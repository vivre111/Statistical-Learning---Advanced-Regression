\documentclass[10pt]{article} 

\usepackage{fullpage}
\usepackage{bookmark}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref} % for the URL
\usepackage[shortlabels]{enumitem}
\usepackage{mathtools}
\usepackage[most]{tcolorbox}
\usepackage[amsmath,standard,thmmarks]{ntheorem} 
\usepackage{physics}
\usepackage{pst-tree} % for the trees
\usepackage{verbatim} % for comments, for version control
\usepackage{tabu}
\usepackage{tikz}
\usepackage{float}

\lstnewenvironment{python}{
\lstset{frame=tb,
language=Python,
aboveskip=3mm,
belowskip=3mm,
showstringspaces=false,
columns=flexible,
basicstyle={\small\ttfamily},
numbers=none,
numberstyle=\tiny\color{Green},
keywordstyle=\color{Violet},
commentstyle=\color{Gray},
stringstyle=\color{Brown},
breaklines=true,
breakatwhitespace=true,
tabsize=2}
}
{}

\lstnewenvironment{cpp}{
\lstset{
backgroundcolor=\color{white!90!NavyBlue},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}; should come as last argument
basicstyle={\scriptsize\ttfamily},        % the size of the fonts that are used for the code
breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
breaklines=true,                 % sets automatic line breaking
captionpos=b,                    % sets the caption-position to bottom
commentstyle=\color{Gray},    % comment style
deletekeywords={...},            % if you want to delete keywords from the given language
escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
% firstnumber=1000,                % start line enumeration with line 1000
frame=single,	                   % adds a frame around the code
keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
keywordstyle=\color{Cyan},       % keyword style
language=c++,                 % the language of the code
morekeywords={*,...},            % if you want to add more keywords to the set
% numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
% numbersep=5pt,                   % how far the line-numbers are from the code
% numberstyle=\tiny\color{Green}, % the style that is used for the line-numbers
rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
showstringspaces=false,          % underline spaces within strings only
showtabs=false,                  % show tabs within strings adding particular underscores
stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
stringstyle=\color{GoldenRod},     % string literal style
tabsize=2,	                   % sets default tabsize to 2 spaces
title=\lstname}                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}
{}

% floor, ceiling, set
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\set}{\lbrace}{\rbrace}
\DeclarePairedDelimiter{\iprod}{\langle}{\rangle}

\DeclareMathOperator{\Int}{int}
\DeclareMathOperator{\mean}{mean}

% commonly used sets
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\renewcommand{\P}{\mathbb{P}}

\newcommand{\sset}{\subseteq}

\theoremstyle{break}
\theorembodyfont{\upshape}

\newtheorem{thm}{Theorem}[subsection]
\tcolorboxenvironment{thm}{
enhanced jigsaw,
colframe=Dandelion,
colback=White!90!Dandelion,
drop fuzzy shadow east,
rightrule=2mm,
sharp corners,
before skip=10pt,after skip=10pt
}

\newtheorem{cor}{Corollary}[thm]
\tcolorboxenvironment{cor}{
boxrule=0pt,
boxsep=0pt,
colback={White!90!RoyalPurple},
enhanced jigsaw,
borderline west={2pt}{0pt}{RoyalPurple},
sharp corners,
before skip=10pt,
after skip=10pt,
breakable
}

\newtheorem{lem}[thm]{Lemma}
\tcolorboxenvironment{lem}{
enhanced jigsaw,
colframe=Red,
colback={White!95!Red},
rightrule=2mm,
sharp corners,
before skip=10pt,after skip=10pt
}

\newtheorem{ex}[thm]{Example}
\tcolorboxenvironment{ex}{% from ntheorem
blanker,left=5mm,
sharp corners,
before skip=10pt,after skip=10pt,
borderline west={2pt}{0pt}{Gray}
}

\newtheorem*{pf}{Proof}
\tcolorboxenvironment{pf}{% from ntheorem
breakable,blanker,left=5mm,
sharp corners,
before skip=10pt,after skip=10pt,
borderline west={2pt}{0pt}{NavyBlue!80!white}
}

\newtheorem{defn}{Definition}[subsection]
\tcolorboxenvironment{defn}{
enhanced jigsaw,
colframe=Cerulean,
colback=White!90!Cerulean,
drop fuzzy shadow east,
rightrule=2mm,
sharp corners,
before skip=10pt,after skip=10pt
}

\newtheorem{prop}[thm]{Proposition}
\tcolorboxenvironment{prop}{
boxrule=0pt,
boxsep=0pt,
colback={White!90!Green},
enhanced jigsaw,
borderline west={2pt}{0pt}{Green},
sharp corners,
before skip=10pt,
after skip=10pt,
breakable
}

\setlength\parindent{0pt}
\setlength{\parskip}{2pt}


\begin{document}
\let\ref\Cref

\title{\bf{STAT 444 Statistical Learning}}
\date{\today}
\author{Austin Xia}

\maketitle
\newpage
\tableofcontents
\listoffigures
\listoftables
\newpage
\section{Course Information}
    \subsection{Contact}
        \begin{center}
            Instructor: Reza Ramezan\\
            Email: rramezan@uwaterloo.ca
        \end{center}
    \subsection{Grade}
        \begin{center}
            Assignments 70\%\\
            Quizzes 30\%\\
        \end{center}
\section{Global modelling methods}
    \subsection{Quick review of linear regression}
        \subsubsection{Simple Linear Regression}
            $$\mu(x_i)=\mu_{0i}+\mu_{1i}(x_i)$$
            regression line is\\
            $$\hat{\mu}(x_i)=\hat{\mu}_{0i}+\hat{\mu}_{1i}(x_i)$$
            $y-\mu(x_i)$ and $y-\hat{\mu}(x_i)$ are not the same thing
        \subsubsection{Multiple Linear Regression}
            $y=\mu(x)+r$ where $\mu(x)=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3$
            here comes p value and t score\\
            $$Y|X=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3+\epsilon $$
            Y is response vector\\$\beta$ is parameter\\$\epsilon$ is error terms\\X is design matrix
        \subsubsection{Least squares estimation}
            \begin{defn}[RSS]
                $$RSS(\beta)=\sum_{i=1}^{n}\left(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij}\right)^2=(y-X\beta)^T(y-X\beta)$$
                SSE and RSS are the same thing, both referring to the Residuals sum of squares\\
            \end{defn}
            \begin{defn}[Ordinary least square estimate]
                $$\hat{\beta}=argmin_\beta (RSS(\beta))=argmin\left((y-X\beta)^T(y-X\beta)\right)$$
            \end{defn}
            $$\frac{d}{d\beta}RSS(\beta)=-2X^T(y-X\beta)$$
            $$X^T(y-X\beta)=0$$
            $$X^Ty=X^TX\beta$$
            $$\beta=(X^TX)^{-1}X^Ty$$
            It's normally asuumed that:\\
            \begin{itemize}
                \item The mean of Y is linear function of X
                \item Error terms have constant mean 0
                \item Error terms have constant variance $\sigma ^2$
                \item Error terms follow a normal distribution
                \item Error terms are independent
            \end{itemize}
            Need these assumptions to do prediction
           \begin{thm}
               Assume we can write y in terms of its mean and an error term
               $$Y=E(Y|X)+\epsilon=\beta_0+\sum\beta_iX_i+\epsilon$$
               where $\epsilon \thicksim MVN(\beta, \sigma^2I_{n*n})$
               then
               $$\tilde{\beta}_{OLS}\sim MVN(\beta, \sigma^2(X^TX)^{-1}) $$
               $$(n-p-1)\frac{\tilde{\sigma}^2}{\sigma^2}\sim \chi^2_{n-p-1}$$
           \end{thm}
            \begin{defn}[The Hat Matrix]
                $$\hat{y}=X\hat{\beta}=X(X^TX)^{-1}X^Ty=Hy$$
            \end{defn}
            We have:
            \begin{itemize}
                \item $H^2=H$
                \item H(1-H)=1
                \item tr(H)=p+1
            \end{itemize}
            {\Large$\hat{r}$ verses $\epsilon$}
            \begin{itemize}
                \item $\epsilon=y-X\beta$
                \item $r=y-\hat{y}=y-X\hat{\beta}$
                \item r is observed residual, $\epsilon$ is model error terms
                \item We will use $\hat{r}$ as approximate values for $\epsilon$ to check if\begin{itemize}
                    \item $E(\epsilon_i)=0$
                    \item $Var(\epsilon_i)=\sigma^2$
                    \item $\epsilon_1, ..., \epsilon_n$ are Normally distrubuted
                    \item $\epsilon_1, ..., \epsilon_n$ are independent
                \end{itemize}
            \end{itemize}


        \subsubsection{Alternative methods to ordinary least squares regression}
            If assumptions on error term don't hold true:
            \begin{itemize}
                \item Use transformation to project onto a space where assumption hold true
                \item Use weighted least squares regression
                \item Use non-parametric models. e.g. kernel regression, splines, etc.
            \end{itemize}
        \subsubsection{Influential Points}
            Based on hat matrix H $\hat{y}=Hy$, we measure influnce by:
            $$\frac{d\hat{y}_i}{dy_i}=H_{ii}$$
            which is influnence of $y_i$ on the data.\\If high leverage points exist in data, one can use robust regression model\\
            Multicolinearity, Curse of Dimensionality
            $(X^TX)^{-1}$ must exsit, so under multicollinearity or $p \geq n$, LS estimation breaks down.
            \\or if rows are closed to linearly dependent, trace of Hat Matrix get to large, causing prediction having variance too large
        \subsubsection{Reducible vs. irreducible errors}
            \begin{itemize}
                \item prediction is for unobserved data, it needs test set 
                \item inference. We would like to answer these questions in light of sampling variability.
            \end{itemize}
            $$MSE=E\left([Y-\hat{Y}]^2|X\right)=\left(f(x)-\hat{f}(x)\right)^2+Var(\epsilon)$$
            the first term is Reducible error, could be 0 if $\hat{f}==f$\\
            the second term is irreducible error. caused by projecting Y into space of X
        \subsubsection{Bias-Variance tradeoff}
            $$E\left([Y_0-\hat{f}(x_0)]^2\right)=E[(f(x_0)-E(\hat{f}(x_0))]^2+E[(\hat{f}(x_0)-E(\hat{f}(x_0))]^2+Var(\epsilon)$$
            which is $$Bias(\hat{f}(x_0))^2 + Var(\hat{f}(x_0))^2+Var(\epsilon)$$
            Bias decrease, flexibility increase as complexity increase\\
            Variance increase, stability decrease as complexity increase
        \subsubsection{Cross-Validation: Basic idea}
            \begin{defn}[Expected error]
                $E(L(Y,\hat{f}(X))$  where $\hat{f}$ is estimate of f, L (loss function) measures distance between Y and $\hat{f}(X)$
                \\eg squared error loss function is defined as $$L(Y,\hat{f}(X))=(Y-\hat{f}(X))^2$$
            \end{defn}
            {\Large Cross Validation forms many test/training sets and measure the prediction error on each test set. The average of these error setimates the expected prediction error}
            
            \begin{defn}
                \begin{itemize}
                    \item Partition data ramdomly to k disjoint and equal-size sets $T_1, ..., T_n$ each of size $n_k=n/k$
                    \item For $ i=1,...,k$ construct training sets $T_{-i}=T_1 ... \cup T_{i-1} \cup T_{i+1} \cup ... T_n$
                    \\ calculate $$sMSE_i = \frac{1}{n_k}\sum_{j\in T_i}\left(y_j-\hat{f}^{-i}(x_j)\right)^2$$
                    where $f^{-i}(x_j)$ is estimate/fitted value of $y_i$ based on a model fitted to $T_{-i}$
                    \item the overall k-fild cross-validation error is 
                    $$CV_k = \frac{1}{k}\sum^k_{i=1}sMSE_i$$
                \end{itemize}
            \end{defn}
            Note: loss function can be replaced
            \\ it can be shown that 
            $$CV_k=\frac{1}{n}\sum_{j=1}^n\left(y_j-\hat{f}^{-\omega(j)}(x_j)\right)^2$$ this formula is understood base on test set\\
            $\omega:{1 ... n} \rightarrow {1 ... k}$ is an indexing function that inducates the partition to which observation j is allocated by the ramdomization 
            \\{\Large choice of k is bias-variance trade-off. Large k results in similar training sets, high variance, smaller bias, more flexibility} 
            \\in practice, k is set to 5/10
        \subsubsection{Leave-One-Out CV: LOOCV}
            \begin{itemize}
                \item k=n in k-fold cross validation. i.e. training sets of size n-1 and test sets of size 1
                \item it has largest k, so very little bias and large variance low prediction power.
                \item it's computationally efficient. For least squres regression, it can be shown that 
                $$CV_n=\frac{1}{n}\sum_{i=1}^nsMSE_i=\frac{1}{n}\sum_{i=1}^n\left(\frac{y_i-\hat{y_i}}{1-H_{ii}}\right)^2$$
                The result above is true for most linear smoothers $\hat{Y}=SY$ under squared error loss, in which case $H_{ii}$ will be repalced by $S_{ii}$
            \end{itemize}
        \subsubsection{Generalized Cross-Validation:GCV}
            A convenient approcimation to LOOCV for linear fitting under squared-error loss. The GCV approximation of the prediction error is 
            $$GCV_n = \frac{1}{n}\sum\left(\frac{y_i-\hat{f}(x_i)}{1-trace(S)/n}\right)$$
            trace(S) is effective number of parameters in the model (degree of freedom). so number of parameter increase then GCV decrease prediction power 
        \subsubsection{Best-subset selection}
            The subset selection is the process of selecting q < p explanatory variates in modelling the function f 
            \\Problem with LS estimate $X\hat{\beta}=HY$
            \begin{itemize}
                \item prediction accurary, LS have low bias, but if not $n \gg n$ they have larger variance.
                \item multicollinearity 
                \item if $p>n$ then LS estimate are not unique 
                \item Model interpretability: including irrelevant variables leads to unneccessary complexity 
            \end{itemize}
            {\Large Best-subset Selection:}

            \begin{itemize}
                \item a total of $2^n=\binom{p}{0} +...+ \binom{p}{p}$ models are to be fitted
                \item An efficient algorithm makes this feasible for p as large as 30-40 
            \end{itemize}
            Steps:
            \begin{itemize}
                \item $M_0$ denote the null model, no predictor, the model simply predicts sample mean 
                \item For $k=1,...,p$: Fit all $\binom{p}{k}$ models that contain exactly k predictors
                \\pick the best among these models. call it $M_k$ 
                \item select the best model among them using cross-vadiated prediction error
            \end{itemize}
        \subsubsection{Forward/Backward-stepwise selection}
            Forward stepwise selection The algorithm:
            \begin{itemize}
                \item Let $M_0$ denote the null model, containing no prectors, predicts sample mean 
                \item For k = 0, ..., p-1\begin{itemize}
                    \item consider all p-k models  that augument the predictors in $M_k$ with k additional predicot 
                    \item choose the best among these p-k models, call it $M_{k+1}$.
                \end{itemize}
                \item select the best model
            \end{itemize}
            Backward stepwise selection the algorithm: every step there is one less predictor
            \\Hybird Approach: \begin{itemize}
                \item both forward and backward 
                \item vriables are added to model, after each new variable, the method may also remove variables 
                \item attempts to mimic subset selection while retaining the computational advantages of forward and backword stepwise selction
            \end{itemize}
            Stepwise methods are more constranit than best subset selection, hence they have lower vairance but perhaps more bias
            \\Shrinkage/regularization methods are computationally efficient. \begin{itemize}
                \item LS: $min_\beta(RSS)$
                \item Shrinkage"$min_\beta (RSS)+\lambda * Pen(\beta)$
                \\Penalizing
            \end{itemize}



    \subsection{Regularized regression models}
        \subsubsection{Weighted Least Squares}
            if we want to give more(or less) weights to different observation if there is Outliers and Heteroscedasticity
            \\Heteroscedasticity (constant variance): $y=X\beta + \epsilon$  
            \\$\epsilon_1,...,\epsilon_n \backsim^{iid} N(0, \sigma^2)  $
            $$WRSS(\beta) = \sum^n_{i=1}w_i\left(y_i-\beta_0-\sum^p_{j=1}\beta_j x_{ij}\right)^2$$
            $$\hat{\beta_{WLS}} = argmin_\beta(WRSS(\beta)) = (X^TWX)^{-1}X^TWy$$
            derivation, see slides
        \subsubsection{choice of W}
            In a word it's inversly proportional to variance of $x_i$
            \begin{itemize}
                \item has to do with variance of y (we have to know the variance(for some measuring device i.e.))
                \item if repeated measurements of Y for each X is available, then $Var(Y_i|X_i)=Var(\epsilon_i)$
                        can be estimated
                \item if we can assume distribution. The variance of proprtions we find should be inversly proportional to the sample size\\
                        so a natrual choice of weights is proportional to the sample size
                
                Choice of W:
                \begin{itemize}
                    \item assumption: $Y_i$:average of $n_i$ repeated measurements.\\$w_i = k n_i$
                \end{itemize}
            \end{itemize}
        \subsubsection{Application of weighted least sqaures}
            \begin{itemize}
                \item Forcused accuracy: assign high weights to some observer
                \item Increased Precision: If we know $Var(\epsilon_i)=\sigma_i^2$, then setting
                    $w=1/\sigma_i^2$ results in heteroscedastic MLE, i.e. $\hat{\beta}_{WLS}=\hat{\beta}_{ML}$
                    \\$\epsilon_1, ..., \epsilon_n \sim N(0, \sigma_i^2)$ give such weight transfer $\epsilon$ to $N(0,1)$
                \item Correlated Noise\\
                    we usually assume $Var(\epsilon|X)$ is diagonal matrix or identity matrix. 
                    \\but there might be off-diagonal element
                    let $\sum$ be the var-covariance matrix. We know $\sum$ is square, symmetric, positive definite
                    $\rightarrow \sum = B B^T$ 
                    \\we can show that Estimating $\beta$ in $Y=X\beta + \epsilon$ translates to OLS estimation of 
                    $\beta$ in $B^{-1}y=B^{-1}X\beta+B^{-1}\epsilon$, in this case, $Var(B^{-1}\epsilon)=Var(\epsilon^*)=I$ 
                    \\we can also show $\hat{\beta}_{OLS}=(X^{*T}X)^{-1}X^{*T}y*=(X^T\sum^{-1}X)^{-1}X^T\sum^{-1}y$
                    this looks like WLS on $y=X\beta + \epsilon$ with weight $w=\sum^{-1}$ with $\sum$ being the variance-covariance matrix
            \end{itemize}
        \subsubsection{Genralized least squares}
            WLS problem can be written as miminizing $(y-X\beta)^TW(y-X\beta)$ for a diagonal matrix W
            \\When W is not a diagonal matrix, it is Genralized LS 
    \subsection{Robust regression and breakdown}
        \subsubsection{motivation}
            \begin{itemize}
                \item Bump rule
                \\apply different power transforms to each coordinate\\
                this removes Outliers
                $$\left(T_{\alpha x}(X_u), T_{\alpha y}\right)$$
                this doesn't work every tcolorboxenvironment
                bump rule to straighten scatter plot
                \item Robust Method \\
                LS estimate of the parameter is obtained by miminizing the loss function 
                $$\sum_{i=1}^nr_i^2=\sum\rho(r_i)$$ with $\rho(r)=r^2$
                \\robust regression choose a $\rho$ that make extreme point less significant
            \end{itemize}
            $$S(\beta)=\sum \rho(r_i)=\sum\rho(y_i-x_i^T\beta)$$
            $$\frac{dS(\beta)}{d\beta}=0 \rightarrow \sum x_i^T\phi (r_i)=0$$
            $\phi(r)$ being $\rho^{'}(r)$, $\rho$ being the loss function
            \\we can let $\rho$ be gentle to outliers to make it robust
            \\closed form means we can isolate $\beta$, otherwise it's not closed form
            \begin{defn}[M-estimator]
                The estimator $\hat{\beta}$ which minimize the function $\sum \rho(r_i)$
                is called an M-estimator which is maxinum likelihood type estimator
            \end{defn}
        \subsubsection{Iteratively Reweighted Least Squares}
            it is one way of solving $0=\sum \phi(r)x$
            \\we can show that$$0=\sum \phi(r)x=\sum\frac{\phi(r_i)}{r_i}(y_i-x_i^T\beta)x_i=
            \sum w(y-x\beta)x=argmin_\beta \sum w(y-x\beta)^2$$ 
            this is same format as weighted least squares
            $w_i=\phi(r_i)/r_i$. $W=diag(w_1...w_n)$ $\hat{\beta}=(x^TWX)^{-1}X^Ty$
            \\problem is that the weight depends on $\beta$, and $beta$ depends on $w_i$
            \\so we give $\beta$ an initial value and calculate weight Iteratively untill convergent
            {\Large the algorithm}
            \begin{itemize}
                \item set initial value for $\beta$
                \item compute Residuals
                \item update the weight $w=\frac{\phi(r)}{r}$, W is $diag(w_1...w_n)$
                \item calculate $\beta^{j+1}=(X^TW^jX)^{-1}X^TW^jy$
                \item j++, return to step 1 converge if $\beta^{j+1}-\beta^j < \epsilon$
            \end{itemize}
            Huber Loss:
            $\rho(r)=0.5r^2$ if $|r| \leq c$, or $c(|r|-0.5c)$ if $|r|>c$ it's quadraitc before c, linear after c
            \\choice of c: if variance 
            \\A $\phi$ function:
            \begin{itemize}
                \item $\phi(-x)=-\phi(x)$
                \item slope is 1 at 0
                \item $\phi(x)\geq 0$ for $x\geq 0$; $\phi(x)>0$ for $0<x<x_r$
                \item it then follows from 1 that $\phi(0)=0$
            \end{itemize}
            In practice, we typically need to scale the residuals, $r_i^*=r_i/S$ S is a scaling parameter
            one choice of S is $MAD=median|r_i|$ and we use $S=MAD/{0.6745}$
        Robust means resistance to outliers 
        \subsubsection{Sensitivity Curve}
            $$SC(y)=\frac{T_N(y_1...y_{N-1},y)-T_{N-1}(y_1...y_{N-1})}{1/N}$$
            we have property: If the SC of function f is the same as $\phi$, the derivative of $\rho$
        . Then we have f(x) minimize the $\rho$
        \subsubsection{breakdown point}
            \begin{defn}
                \begin{itemize}
                    \item $Z_m$ be replacing m of $z_i$ with random desired number 
                    \item $e(m,T,Z)=sup_{Z^*_m}|T(Z^*_m)-T(Z)|$
                    \item breakdown point is $min\{m/n:e(m,T,Z)=\infty\}$
                \end{itemize}
            \end{defn}
        we want breakdown point to be as large as possible 
        \\we can change LS to least trimmed sqaures
        $$\hat{\beta}_{LTS}=argmin_\beta(TrimAverage(y_i-x_i^T\beta)^2)$$ this way the break down point = $\frac{n-k+1}{n}$

\section{Locally adaptive methods(smooth functions)}
    \subsection{Local linear regression}
        \subsubsection{Introduction to local regression}
            $$y=\{^{\beta_1 x\  if\  x \leq a}_{\beta_2x\ if\ x\geq a}$$
            subject to $\beta_1 a=\beta_2a$\\
            we can have continuity condition or no continuity condition
            $$y=\beta_0+\beta_1x+\beta_2(x-a)I(x\geq a)$$
            we can have differentiability condition:
            $$\alpha_1+2\alpha_2 a=y_1+2y_2a$$
            quadraitc piecewise model can be written as
            $$y=\beta_0+\beta_1x+\beta_2x^2+\beta_3(x-a)^2I(x\geq a)$$
            this model is continuous differentiabile 
            \begin{defn}[smooth]
                twice differentiabile
            \end{defn}
            knots=change-point\\
            some issue:\begin{itemize}
                \item the location of the intersecting point (knot) may not be known 
                \item how many knot
                \item linear model assumption such as constant variance may not hold (completely different variance before and after a) 
                \item we can solve outlier and suddenly changing variance by piecewise model
                \item the effect of outliers on fitting can be substituted 
            \end{itemize}
   \subsection{model for subset of of data}
        create subsets of points close to each other 
        \begin{itemize}
            \item natrual way of avoiding strong parametric model 
            \item for Example fit only simple linear model
            \item removes influnce of Outliers
        \end{itemize}
        defining a neighbourhood:\\
        For scaler explanatory variate $x\in R$
        \\neighbourhood of x is $\{x_i"|x_i-x|\leq \delta, \forall i=1...n\}$
        \\or the $||x-x_i||$ distance\\\\
        k neighbourhood: fixed neighbourhood size, find k nearest neighbours
        \\or $k\%$ nearest neighbours
        package FNN
        \\KNN local regression:
        \begin{itemize}
            \item gather a fraction $s=k/n$ training points whose $x_i$ are closest to $x_0$
            \item assign weight $K_{i0}=1$ to point in this neighbourhood, zero weight elsewhere
            \item $$\hat{beta}=argmin\sum K_{i0}\rho(y_i-x^T_i\beta)$$
            \item the fitted value at $x=x_0$ is $\hat{f}(x_0)=x_0^T\hat{\beta}$
        \end{itemize}
        Genralization:\\
        \begin{itemize}
            \item replacing 0-1 with weight function 
            \item choose robust loss functions for $\rho(r)$ can improve the fit 
        \end{itemize}
        summary:
        \begin{itemize}
            \item local fitting is only fitting the data locally.
            \item remove influnce of faraway points 
            \item can have robust local methods (using robust$\rho$)
        \end{itemize}
    \subsection{Smoothing splines}
        \subsubsection{Geometry of Polynomial Regression}
            Before we have $\mu(x)\beta_0+\beta_1x+\beta_2x^2...$\\
            we can also have $\mu(x)=\theta_1g_1(x)+\theta_2g_2(x)...$
            \begin{itemize}
                \item all linear combinations of these $g(x)$ form a subspace
                \item $g_i$ are generators of that subspace 
                \item If the generators are lienarly independent of one another
                \\Then these functions also form a set of orthogonal basis function for that subspace 
                \item The model asserts $\mu(X)$ lies in that subspace, subspace dimention equals the number of basis functions which defines it 
                
            \end{itemize}
            $$y=\beta_0+\beta_1x_1+\beta_2x_2$$$$y=\delta_0+\delta_1(x_1-x_2)+\delta_2(x_1+x_2)$$
            the latter one is using orthogonal function. the basis the parameter are different
        \subsubsection{Linear Basis Expansion}
        \begin{itemize}
            \item Piecewise constant regression under $N_\delta$ and KNN 
            \item Piecewise linear regression under $N_\delta$ and KNN 
            \item continuity restriction
        \end{itemize}
        $$Y|X=\mu(X)+\epsilon=\sum \beta_mh_m(X)+\epsilon$$
        Question: how to choose the basis 
        Answer:\begin{itemize}
            \item choose them manually beforehand to limit the class of functions 
                $$\mu(X)=\sum\mu_j(X_j)=\sum\sum\beta_{jm}h_{jm}(X_j)$$
            \item include all and use a varialbe selection procedure
            \item include all and regularize 
            $$min_\beta [\sum\rho(r_i)+\lambda\sum\beta_j^2]$$
        \end{itemize}
        \subsubsection{degree of freedom}
            b-spline is more stable\\
            number of constranit = k continuity + k derivatrive + k 2nd derivative\\
            number of parameter = (k+1)*(degree)\\
            df=k+4
        \subsubsection{Smoothing spline}
            decide on number and location of the knots\\
            we define a roughness penalty
            $$RSS(\mu, \lambda)=\sum^n_{i=1}(y_i-\mu(x_i))^2+\int(\mu^{''}(t))^2dt$$
            \begin{thm}
                suppose f(x) is a real function whose value is known only at $x_1 ... x_n$. The points 
                $(x_1, f(x_1))$ can be ussed to determine natrual cubic splines s(x) s.t. 
                $s(x_i)=f(x_i)$ it can be shown that 
                $$\int(s^{''}(x))^2dx\leq \int(g^{''}(x))^2dx$$ 
            \end{thm}
            we want to find $$min_\mu\sum^n_{i=1}(y_i-\mu(x_i))^2+\int(\mu^{''}(t))^2dt$$
            \\the solution requires $\hat{\mu}(x)$ to be $\hat{\mu}(x)=\sum N_j(x)\hat{\beta_j}$, wehre $N_j(x)$ are a set of n basis functions for the family of natrual cubic splines
            \\now we try to find $\mu$, the integer is rewritten as  
            $$\int (\sum_{j=1}\beta_jN_j^{''}(x))^2dx=\beta^T\omega_N\beta $$
            $\omega=[W_{ij}]$ an n*n matrix, wehre $W_{ij}=\sum N^{''}_i(x)N^{''}_j(x)dx$
            \\$\hat{\mu} = N \hat{\beta} = N(N^T+\lambda\Omega_N)^{-1}N^Ty = S_\lambda y$
        \subsubsection{choosing $\lambda$}
            $\hat{\mu} = N \hat{\beta} = N(N^T+\lambda\Omega_N)^{-1}N^Ty = S_\lambda y$
            $$df_\lambda = trace(S_\lambda)=\sum \{S_\lambda\}$$
            \begin{itemize}
                \item LOOCV 
                $$RSS_{CV}(\lambda)=\sum\left(\frac{y_i-\hat{\mu}_\lambda (x_i)}{1-\{S_\lambda\}_{ii}}\right)^2$$
                \item Generalized CV 
                $$RSS_{GCV}(\lambda)=\sum\left(\frac{y_i-\hat{\mu}_\lambda (x_i)}{1-\frac{1}{n}trace(S_\lambda)}\right)^2$$
            \end{itemize}
        \subsubsection{Eigen Decomposition}
        $\rho_1\geq \rho_2...\geq 0$ be eigen values corresponding to eigenvector $u_1, ...u_n$
            $$H=\sum u_i \rho_i u_i^T y = \sum u_i \rho_i <u_i, y>$$
            For smooth-splines: 
            $$\hat{\mu}=S_\lambda y\ where\ S_\lambda = N(N^TN+\lambda \Omega_N)^{-1}N^T$$
            $$S_\lambda = (I_n+\lambda K )^{-1}\ where\ K=N^{-T}\Omega_NN^{-1}$$
            $$S_\lambda=(I_n+\lambda K)^{-1}=(I_n+\lambda VDV^T)^{-1}=V(I_n+\lambda D)^{-1}V^T$$
            this is great then eigenvalues is $$\rho_i(\lambda)=\frac{1}{1+\lambda d_{n-i+1}}$$
            and $\hat{\mu}= \sum \rho_i(\lambda)v_i<v_i, y>$
        \subsubsection{Multidimensional Splines}
        how to git a curve to multiple variates? here are several ways:
            \begin{itemize}
                \item Using tensor product basis 
                \item Using a multivariate high curvature penalty: thin plate splines 
                \item Using an additive model
            \end{itemize}
        \subsubsection{Tensor Product}
            $$g_{jk}(x_1,x_2)=h_{1j}(x_1)h_{2k}(x_2), j=1...M_1, k=1,...M_2$$
            $$\mu(x_1,x_2)=\beta_0+\sum_j\sum_k\beta_{jk}g_{jk}(x_1,x_2)$$
        \subsubsection{thin plate splines}
            solve $$min\{\sum(y_i-\mu(x_i))^2+\lambda J[\mu]\}$$
            $J[\mu]$ is approperiate penalty function in $R^d$ ($x_i \in R^d$) 
            $$\mu(x)=\beta_0+\beta^Tx+\sum\alpha_jh_j(x)$$
            where 
        \subsubsection{additive spline model}
            $$\mu(x)=\beta_0+\mu_1(x_1)...+\mu_d(x_d)$$
            $$J[\mu]=\sum_{j=1} \int \mu_j^{''}(t_j)dt_j$$
            
        \subsubsection{Moving beyond linearity}
    \subsection{Kernel method}
        \subsubsection{Local weighting}
            we choose weight function K(t) s.t.
            \begin{itemize}
                \item $\int K(t)dt=1$
                \item $\int tK(t)dt=0$
                \item $\int t^2K(t)dt<\infty$
            \end{itemize}
            we evaluate kernel function at $\frac{x_i-x}{h}$
            x is the location
            $$w(x,x_i)=\frac{K(\frac{x_i-x}{h})}{\sum K(\frac{x_j-x}{h})}$$
        \subsubsection{smoothing matrix svd}\
            $$\hat{\mu}=UD_\rho V^Ty=\sum_{i=1}^nU_i\rho _i<V_i,y>$$
            this separates into basis vectors $U_i$, singular values $\rho_i$ and 
            rothogonal component of y along direction vectors $V_i$
            \begin{itemize}
                \item coefficients of y: higher if x is closer to value of $x_i$
                \item singular values: have elbow shape, where singular values die off quickly.
                \item y compononents: $<V_i,y>$:similar pattern to singlar values 
                \item basic functions: increase in complexity as i increase and higher frequency basis functions have small singular values and y components
            \end{itemize}
    \subsection{Density/intensity function estimate}
        \subsubsection{Lasso and Ridge}
            $$\hat{\beta}^{lasso}=argmin_\beta\{\sum_{i=1}^n(y_i-\beta_0-\sum x_{ij}\beta_j)^2+\lambda\sum|\beta_j|\}$$
            $$\hat{\beta}^{ridge}=argmin_\beta\{\sum_{i=1}^n(y_i-\beta_0-\sum x_{ij}\beta_j)^2+\lambda\sum(\beta_j)^2\}$$
        \subsection{kernel density estimation}
            $$\hat{f_U(u)}\approxeq \frac{num of u_i in (u-h, u+h)}{2nh} $$
            $$\hat{f_U(u)}\approxeq \frac{1}{nh}\sum_i^nK(\frac{u-u_i}{h}) $$
            To choose h we use:
            \begin{itemize}
                \item pesudo-likelihood $$PL(h)=\prod_i^n\hat{f}_{-i}(u_i)$$
                \item MISE mean intergrated squared error 
                $$MISE(h)=E(ISE(h))=\int E(\hat{f}(x)-f(x))^2dx =\int\{Var(\hat{f}(x)+Bias^2\hat{f}(x))\}dx$$
            \end{itemize}
\section{Preditive accuracy}
    \subsection{Roles of training and test data, cross-validation, etc}
    \subsection{Bias/Variance trade-off and parameter choice}
\section{Locally adaptive methods(tree-based methods)}
    \subsection{regression trees}



\end{document}
